{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Learn the basics about notebooks and Apache Spark\n\nThis notebook introduces you to the basics of analytics notebooks and explains what Apache Spark is and how to use Spark in notebooks. The notebook shows you how to load data into the notebook, parse and explore the data, run queries on the data to extract information, plot your analysis results, and save your result in Object Storage.\n\nThis notebook runs on Python 3.7 with Spark 3, and Cloud Object Storage.\n\n## Table of contents\n- [What is Apache Spark](#apache_spark)\n- [Employee Attrition Data](#Employee_Attrition_Data)\n- [Load data](#load_data)\n- [Working with RDD](#RDD)\n- [Work with DataFrame](#DataFrame)\n- [Use Spark SQL](#use_spark_sql)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"apache_spark\"></a>\n## What is Apache Spark\n\n[Spark](http://spark.apache.org/) is a fast open-source engine for large-scale data processing. It is built for speed and ease of use. Through the advanced DAG execution engine that supports cyclic data flow and in-memory computing, programs can run up to 100 times faster than Hadoop MapReduce in memory, or 10 times faster on disc.\n\nSpark consists of the following components:\n\n* Spark Core is the underlying computation engine with the fundamental programming abstraction called resilient distributed datasets (RDDs)\n* Spark SQL provides a new data abstraction called DataFrame for structured data processing with SQL and domain-specific language\n* MLlib is a scalable machine learning framework for delivering fast distributed algorithms for mining big data\n* Streaming leverages Spark's fast scheduling capability to perform real-time analysis on streams of new data\n* GraphX is the graph processing framework for the analysis of graph structured data\n\n<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"50%\" height=\"50%\"></img>\n\nThe Apache Spark driver application uses the predefined SparkContext object to allow the interaction with the driver application. The SparkContext object tells Spark how and where to access a cluster."}, {"metadata": {}, "cell_type": "markdown", "source": "To check the Spark version, run the sc.version command"}, {"metadata": {}, "cell_type": "code", "source": "sc.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"Employee_Attrition_Data\"></a>\n## Employee Attrition data\nIn this notebook, you will focus on Spark Core and Spark SQL by using the Python API. You will analyze the employee attrition data at https://www.kaggle.com/analystanand/employee-attrition/data.\n\n`[, 0]\tID                     Employee Number`  \n`[, 1]\tsatisfaction_level     Job Satisfaction Level`  \n`[, 2]\tlast_eval_rating\t   Time since last evaluation in years`  \n`[, 3]\tprojects_worked        Number of projects completed while at work`  \n`[, 4]\taverage_monthly_hours  Average number of working hours per month`  \n`[, 5]\ttime_spend_company     Time spent at the company in years`  \n`[, 6]\twork_accident          Whether the employee had a workplace accident`  \n`[, 7]\tpromotion_last_5year   Was an employee promoted over the last 5 years`  \n`[, 8]\tDepartment             Department in which an employee works`  \n`[, 9]\tsalary                 Salary level (low, medium, high)`  \n`[,10]\tAttrition              0=Stayed  1=left the workplace`"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load_data\"></a>\n## Load data\nTo load the CSV file to your notebook: \n\n1. Click the **Data** icon on the notebook action bar. \n2. Click on **browse** to add empattrition.csv file.\n\nThe data file is now listed on the **Files** tab of the **Data** panel and is stored in Object Storage.\nPlace the cursor in the code cell below.  Click on an arrow next to the file name in the right panel and select Insert SparkSession DataFrame."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "markdown", "source": "**Navigate to Manage -> Environmental Runtimes and record 1) Total account capacity unit hours used and 2) Remaining account capacity unit hours on the Spark Tutorial answer sheet.**"}, {"metadata": {}, "cell_type": "markdown", "source": "# <a id=\"RDD\"></a>\n## Working with RDD"}, {"metadata": {}, "cell_type": "markdown", "source": "We use `SparkContext` to load the data into a `Spark RDD` named `ea`.\nResilient Distributed Dataset (RDD) is a collection of elements that can be operated on in parallel. RDDs are immutable.  An update requires creating a new RDD.  The Spark driver application distributes the work across the cluster.\n\nYou can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.\n\nYou can run these types of methods on RDDs: \n - Actions: query the data and return values\n - Transformations: manipulate data values and return pointers to new RDDs. \n\nFind more information on Python methods in the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html\" target=\"_blank\" rel=\"noopener noreferrer\">PySpark documentation</a>."}, {"metadata": {}, "cell_type": "code", "source": "#Replace 'BUCKET' with the bucket name for your project\nea = sc.textFile(cos.url('empattrition.csv', 'BUCKET'))\nea.take(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Instantiate RDD"}, {"metadata": {}, "cell_type": "markdown", "source": "You can now access the data by using the preconfigured `SparkContext` function in your notebook."}, {"metadata": {}, "cell_type": "markdown", "source": "The RDD you created is a collection of strings corresponding to the individual lines in the raw data file. It is also important to remember that the RDD is defined but not instantiated. By applying an action like `count` to the RDD, you effectively instantiate the RDD."}, {"metadata": {}, "cell_type": "code", "source": "# We subtract 1 from the count to obtain the number of employees because the first row is the column headings\nprint (\"Total number of employee records:\", ea.count()-1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Apply another action to the same RDD.  Read the first row."}, {"metadata": {}, "cell_type": "code", "source": "print (\"The first row:\", ea.first())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Parse Data"}, {"metadata": {}, "cell_type": "markdown", "source": "To begin working with the data, you need to parse it into columns. You can do this by mapping each line in the RDD to a function that splits the line by commas.\n\nmap(func): transformation function that returns a new RDD with the results of running the specified function on each element\n\nThe lambda notation in Python is used to create anonymous functions which are not bound to a name. The function is passed as a parameter to the `map` function. The anonymous function splits each line from the `ea` RDD  at comma boundaries. Hence, the new `eaParse` RDD is a list of sub-lists. Each parent list in `eaParse` corresponds to a line in `ea`. The strings in each sub-list are the individual row elements."}, {"metadata": {}, "cell_type": "code", "source": "eaParse = ea.map(lambda line : line.split(\",\"))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "raw", "source": "Check the first row in eaParse RDD"}, {"metadata": {}, "cell_type": "code", "source": "# The first row in eaParse RDD contains the column names\neaParse.first()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The row elements are numbered starting from zero.  Display the first element in the first row."}, {"metadata": {}, "cell_type": "code", "source": "eaParse.first()[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Display the third element in the first row"}, {"metadata": {}, "cell_type": "code", "source": "eaParse.first()[2]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Display the last element.  -1 is the position of the last element"}, {"metadata": {}, "cell_type": "code", "source": "eaParse.first()[-1]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Filtering Data\nfilter(func): transformation function that returns a new RDD with the elements for which the specified function is true"}, {"metadata": {}, "cell_type": "code", "source": "# eaSatisfaction is a copy of eaParse with the first row removed\n#You need to specify a new RDD name because RDDs are immutable\neaSatisfaction = eaParse.filter(lambda x: x[0] !=\"ID\")\neaSatisfaction.take(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Calculate the average job satisfaction per department"}, {"metadata": {}, "cell_type": "markdown", "source": "The `eaSatisfaction` RDD contains a list of pairs (v1, v2), where v1 is a department name and v2 is a job satisfaction for one employee. Table 1. illustrates this structure."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Table 1.\n\n<table border=1 style=\"width:80%\" align=\"left\">\n  <tr>\n    <th>Key</th><th>Value</th>\n  </tr>\n  <tr>\n    <td>Sales</td><td>Value 1</td>\n  </tr>\n  <tr>\n    <td> IT</td><td>Value 2</td>\n  </tr>\n    <tr>\n    <td>Sales</td><td>Value 3</td>\n  </tr>\n    <tr>\n    <td>IT</td><td>Value 4</td>\n  </tr>\n    <tr>\n    <td>hr</td><td>Value 5</td>\n  </tr>\n  <tr>\n    <td>...</td><td>...</td>\n  </tr>\n</table>\n<p>"}, {"metadata": {}, "cell_type": "markdown", "source": "Transform (map) this data set into a new one where each row (data pair) is augmented with the value `1`. Table 2. shows this new structure."}, {"metadata": {}, "cell_type": "code", "source": "eaCountByKey = eaSatisfaction.map(lambda x : (x[8], (float(x[1]), 1)))\neaCountByKey.take(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Table 2.\n\n<table border=\"1\" style=\"width:80%\" align=\"left\">\n  <tr>\n    <th>Key</th><th>Value</th>\n  </tr>\n  <tr>\n    <td>Sales</td><td>(Value 1,1)</td>\n  </tr>\n  <tr>\n    <td>IT</td><td>(Value 2,1)</td>\n  </tr>\n    <tr>\n    <td>Sales</td><td>(Value 3,1)</td>\n  </tr>\n    <tr>\n    <td>IT</td><td>(Value 4,1)</td>\n  </tr>\n    <tr>\n    <td>hr</td><td>(Value 5,1)</td>\n  </tr>\n  <tr>\n    <td>...</td><td>...</td>\n  </tr>\n</table>\n<p>"}, {"metadata": {}, "cell_type": "markdown", "source": "Reduce the data representation in Table 2 into the Table 3 representation."}, {"metadata": {}, "cell_type": "code", "source": "eaAddByKey = eaCountByKey.reduceByKey(lambda v1,v2 : (v1[0]+v2[0], v1[1]+v2[1]))\neaAddByKey.take(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Table 3.\n\n<table border=\"1\" style=\"width:80%\" align=\"left\">\n  <tr>\n    <th>Key</th><th>Value</th>\n  </tr>\n  <tr>\n    <td>Sales</td><td>(Value 1 + Value 3,2)</td>\n  </tr>\n  <tr>\n    <td>IT</td><td>(Value 2 + Value 4,2)</td>\n  </tr>\n    <tr>\n    <td>hr</td><td>(Value 5,1)</td>\n  </tr>\n  <tr>\n    <td>...</td><td>...</td>\n  </tr>\n</table>\n<p>"}, {"metadata": {}, "cell_type": "markdown", "source": "Compute the average job satisfaction per department. Create the `eaAverages` RDD by mapping the `eaAddByKey` RDD through a function that divides the sum of satisfaction scores by number employees in the department."}, {"metadata": {}, "cell_type": "code", "source": "eaAverages = eaAddByKey.map(lambda k: (k[0], k[1][0] / float(k[1][1] ) ) )\neaAverages.take(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Print the department names and corresponding average job satisfaction"}, {"metadata": {}, "cell_type": "code", "source": "for pair in eaAverages.top(10):\n    print (\"Department: \", pair[0], \" Average Satisfaction Score: \", pair[1])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Plot the results"}, {"metadata": {}, "cell_type": "code", "source": "department=[]\nsatisfaction=[]\nfor pair in eaAverages.top(10):\n    department.append(pair[0])\n    satisfaction.append(pair[1])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#import the matplotlib pyplot module and specify display plots inline\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN = 10\nindex = np.arange(N)  \nbar_width = 0.5\n\nplt.bar(index, satisfaction, bar_width,\n                 color='r')\nplt.xlabel('Department')\nplt.ylabel('Average Satisfaction')\nplt.title('Average Job Satisfaction per Department')\nplt.xticks(index + bar_width/2, department, rotation=90)\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"DataFrame\"></a>\n## Working with DataFrame\nA DataFrame is a two-dimensional data structure organized into rows and named columns.\nWe will explore the data loaded in df_data_1 DataFrame.  We start from inspecting the data."}, {"metadata": {}, "cell_type": "markdown", "source": "### Inspecting Data"}, {"metadata": {}, "cell_type": "code", "source": "# Number of rows in a DataFrame\nprint('Number of employees:', df_data_1.count())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# The columns list\ndf_data_1.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Number of columns\nlen(df_data_1.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Returns the columns and their data types\ndf_data_1.printSchema()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Print the (logical and physical) plans\ndf_data_1.explain()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#returns first observation\ndf_data_1.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Returns the first 2 observations\ndf_data_1.head(2)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#Returns the first 2 rows.  The data is displayed under the column names\ndf_data_1.show(2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#summary statistics for 1 variable\ndf_data_1.describe( 'satisfaction_level').show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#summary statistics for 2 variables\ndf_data_1.describe( 'satisfaction_level', 'time_spend_company').show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Describe for non-numeric column -  Minimum and maximum is based on ASCI value\ndf_data_1.describe( 'Department').show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## <span style=\"color:red\">Graded Exercise 1</span>\nWrite and run the code to show the summary statistics for average_monthly_hours"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Selecting Data"}, {"metadata": {}, "cell_type": "code", "source": "#Show the specified columns\ndf_data_1.select('Department', 'satisfaction_level', 'salary', 'Attrition').show(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:red\">Graded Exercise 2</span>\nWrite and run the code to select ID, department, salary, Attrition and show the first 5 rows."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Rename the column in the output.  For instance, abbreviate satisfaction_level as sl \ndf_data_1.select('Department', 'satisfaction_level', 'salary', 'Attrition').withColumnRenamed('satisfaction_level', 'sl').show(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# List the distinct values.  For instance, list the department names\ndf_data_1.select('Department').distinct().show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:red\">Graded Exercise 3</span>\nWrite and run the code to display the distinct salary values."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Return the number of distinct values in a column.  For instance, return the number of departments\ndf_data_1.select('Department').distinct().count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# First 20 employees with the job satisfaction above 9.8\ndf_data_1.select('Department', 'satisfaction_level', 'salary', 'Attrition').filter(df_data_1['satisfaction_level']>9.8).show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:red\">Graded Exercise 4</span>\nModify and run the code from Graded exercise 2 to return the ID, department, salary, Attrition for the first 5 employees in hr department."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Department name ends with 'ing'\ndf_data_1.select('Department', 'satisfaction_level', 'salary', 'Attrition').filter(df_data_1.Department.endswith(\"ing\")).show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Satisfaction between 0.1 and 1\ndf_data_1.select('department', 'satisfaction_level', 'salary', 'Attrition').filter(df_data_1.satisfaction_level.between(0.1, 1)).show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Pairwise Frequency"}, {"metadata": {}, "cell_type": "code", "source": "#Pairwise Frequency for categorical variables\ndf_data_1.crosstab('Department', 'salary').show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_data_1.crosstab('attrition', 'salary').show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_data_1.crosstab('attrition', 'promotion_last_5years').show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### GroupBy"}, {"metadata": {}, "cell_type": "code", "source": "#Number of employees in each department\ndf_data_1.groupby('Department').count().show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#sort the output by count descending\ndf_data_1.groupby('Department').count().sort('count', ascending=False).show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Average job satisfaction in each department\ndf_data_1.groupby(['Department'])\\\n.agg({\"satisfaction_level\": \"AVG\"}).show()\n# Average job satisfaction in each department per salary level\ndf_data_1.groupby(['Department', 'salary'])\\\n.agg({\"satisfaction_level\": \"AVG\"}).show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:red\">Graded Exercise 5</span>\nWrite and run the code to return the average job satisfaction for each salary level.  Display the salary level and corresponding average satisfaction."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Plot Average Job Satisfaction per department"}, {"metadata": {}, "cell_type": "code", "source": "dasl=df_data_1.groupby(['Department'])\\\n.agg({\"satisfaction_level\": \"AVG\"}).collect()\n\ndepart=[]\nsatisfaction=[]\nfor row in dasl:\n     depart.append(row[\"Department\"])\n     satisfaction.append(row[\"avg(satisfaction_level)\"])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN = 10\nindex = np.arange(N)  \nbar_width = 0.5\n\nplt.bar(index, satisfaction, bar_width,\n                 color='b')\nplt.xlabel('Department')\nplt.ylabel('Average Satisfaction')\nplt.title('Average Job Satisfaction per Department')\nplt.xticks(index + bar_width/2, depart, rotation=90)\nplt.show()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"use_spark_sql\"></a>\n## Use Spark SQL\n\n`Spark SQL` lets you query structured data, for example, data in a relational table and can be a very powerful tool for performing complex aggregations.\n\nTo create a relational table that you can query using `Spark SQL` and fill it with employee data, you'll use the `Row` class from the `pyspark.sql` package. You will use every line in the `df_data_1` DataFrame to create a row object. Each of the row's attributes will be used to access the value of each column."}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SQLContext, Row\nsqlContext = SQLContext(sc)\ndf_data_1.registerTempTable(\"ea\")\nsqlContext.cacheTable(\"ea\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Column 1 - Department\n# Column 2 - Number of employees who worked for the department in column 1 and left\nempl1 = sqlContext.sql(\"SELECT department, COUNT(*) AS numemp FROM ea WHERE Attrition=1 GROUP BY department ORDER BY upper(department)\").collect()\nfor row in empl1:\n    print (row)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Column 1 - Department\n# Column 2 - Number of retained employees from the department in column 1\nempl0 = sqlContext.sql(\"SELECT department, COUNT(*) AS numemp FROM ea WHERE Attrition=0 GROUP BY department ORDER BY upper(department)\").collect()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for row in empl0:\n    print (row)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "department=[]\nAttrition0=[]\nAttrition1=[]\nfor row in empl0:\n     department.append(row.department)\n     Attrition0.append(row.numemp)\n\nfor row in empl1:\n    Attrition1.append(row.numemp)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use the bar() function to plot a vertical bar chart."}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN=10\nind=np.arange(N)\nwidth = 0.35\nea0 = plt.bar(ind, Attrition0, width, color='g', label='Retained')\nea1 = plt.bar(ind+width, Attrition1, width, color='y', label='Left')\nplt.ylabel('Number of Employees')\nplt.xlabel('Department')\nplt.title('Number of Retained and Not Retained Employees per Department', fontsize=15)\nplt.xticks(ind+width/2, department, rotation='vertical')\nplt.legend()\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can create a pie chart with the pie() function. The function does not display legends or label names by default. You can use the input parameters to change the default colors, set legends, labels, the starting angle, shadowing, and whether any section is exploded.\n\nThe default starting angle is 0, which starts the first slice on the x-axis. If you set startangle=90, the first slice starts on the positive y-axis."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\n\ndata = Attrition1  \nlabels = department\nplt.figure(1, figsize=(6,6))  # make it square\ncolors = ['yellowgreen', 'gold', 'lightskyblue', \n          'lightcoral', 'green', 'red', 'purple', 'cyan', 'magenta', 'orange']\nplt.pie(data, labels=labels, colors= colors, startangle=90)\nplt.title('Attrition by Department')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:red\">Graded Exercise 6</span>\nUse Spark SQL to return <br>\nColumn 1 - salary level <br>\nColumn 2 - number of employees with a salary level in column 1"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "markdown", "source": "**Navigate to Manage -> Environmental Runtimes and record 1) Total account capacity unit hours used and 2) Remaining account capacity unit hours on the Spark Tutorial answer sheet. 3) Answer the follow up questions on the assignment answer sheet**"}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:Blue\">Ungraded Exercise 1</span>\nUse Spark SQL to return <br> \nColumn 1 - department name <br>\nColumn 2 - salary level  <br>\nColumn 3 - number of retained employees who work for the department in column 1 and have a salary level in column 2 <br>\nSort the retuned data by department name"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:Blue\">Ungraded Exercise 2</span>\nUse Spark SQL to return <br> \nColumn 1 - department name <br>\nColumn 2 - salary level  <br>\nColumn 3 - attrition<br>\nColumn 4 - number of employees who work/worked for the department in column 1 with a salary level in column 2 and attrition status in column 3 <br>\nSort the retuned data by department name"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:Blue\">Ungraded Exercise 3</span>\nUse Spark SQL to return <br> \nColumn 1 - department name <br>\nColumn 2 - number of retained employees from department in column 1 <br>\nColumn 3 - number of employees who left from department in column 1 <br>\nColumn 4 - total number of employees from department in column 1 <br>\nSort the retuned data by department name"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python37", "display_name": "Python 3.7 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.7.9", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}
{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Predict Survival of the Titanic Passengers"}, {"metadata": {}, "cell_type": "markdown", "source": "You will use Naive Bayes method to predict if a passenger survived the Ill-fated voyage of the Titanic (1912). The passenger data includes name, age, ticket class, and sex.  If you watched the Titanic movie, you might recall that the first class passengers and women with children received preference to lifeboats.  Hence, gender, age, and ticket class could be the key predictors of survival."}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of Contents\n- [Titanic Data](#Titanic_Data)\n- [Load Libraries](#load_libraries)\n- [Access data](#access_data)\n- [Explore Data](#Explore)\n- [Parse Data](#Parse)\n- [Split Data into Training and Test set](#training_test)\n- [Build Naive Bayes Model](#build_model)\n- [Predict for Test data](#test_data)\n- [Evaluate the Model](#evaluate_model)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"Titanic_Data\"></a>\n## Titanic Data \nYou will analyze the random sample of the Titanic passengers data. The Dataset Source: [https://ww2.amstat.org/publications/jse/v3n3/datasets.dawson.html](https://ww2.amstat.org/publications/jse/v3n3/datasets.dawson.html)"}, {"metadata": {}, "cell_type": "markdown", "source": " <table style=\"font-size: 16px; text-align: left;\" width=100%>\n  <tr>\n  <td width=5% style=\"text-align: center; font-size: 16px\">\n   </td>\n   <td width=11% style=\"text-align: left; font-size: 16px\">\n   <b>Variable</b>\n   </td>\n   <td width=53% style=\"text-align: left; font-size: 16px\">\n   <b> Description</b>\n   </td>\n    <td width=31% rowspan=7>\n \n   <img src='https://www.khaskhabar.com/images/picture_image/3690-titanic-ship.jpg?raw=true'></img>\n  \n  </td>\n  </tr>\n  <tr>\n   <td width=5% style=\"text-align: center; font-size: 16px\">\n     0 \n   </td>\n   <td width=11% style=\"text-align: left; font-size: 16px\">\n     Name \n   </td>\n   <td width=53% style=\"text-align: left; font-size: 16px\">\n     Passenger's first and last name. \n   </td>\n  \n </tr>\n <tr>\n   <td style=\"text-align: center; font-size: 16px\">\n    1\n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n   PClass \n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n   Ticket class (1st, 2nd, or 3rd) based on socio-economic status\n  </td>\n </tr>\n <tr>\n  <td style=\"text-align: center; font-size: 16px\">\n   2 \n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n  Age \n  </td>\n  <td  style=\"text-align: left; font-size: 16px\">\n  Passenger's estimated age in years\n  </td>\n </tr>\n <tr>\n  <td style=\"text-align: center; font-size: 16px\">\n  3 \n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n  Sex \n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n  male or female\n  </td>\n </tr>\n <tr>\n  <td style=\"text-align: center; font-size: 16px\">\n  4 \n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n  Survived\n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n  Indicates if the passenger survived the sinking of the Titanic (1=survived; 0=died)\n  </td>\n </tr>\n <tr>\n  <td style=\"text-align: center; font-size: 16px\">\n   5\n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n  PersonID \n  </td>\n  <td style=\"text-align: left; font-size: 16px\">\n  Passenger's unique identifier\n  </td>\n </tr>\n</table>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load_libraries\"></a>\n## Load Libraries\n\nThe Spark and Python libraries that you need are preinstalled in the notebook environment and only need to be loaded.\n\nRun the following cell to load the libraries you will work with in this notebook:"}, {"metadata": {}, "cell_type": "code", "source": "# PySpark Machine Learning Library\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import  NaiveBayes, MultilayerPerceptronClassifier\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row, SQLContext\n\nimport os\nimport sys\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nfrom pyspark.mllib.regression import LabeledPoint\nfrom numpy import array\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n# Library for confusion matrix, precision, test error\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n# Library For Area under ROC curve and Area under precision-recall curve\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\n# Assign resources to the application\nsqlContext = SQLContext(sc)\n\n# packages for data analysis\nimport numpy as np\nimport pandas as pd", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# The data will be loaded into an array.\n# This is the summary of the data structure, including the column position and name.\n# The first filed starts from position 0. \n\n# 0 Name    -  Passenger first and last name.\n# 1 PClass  -  Ticket class (1st, 2nd, or 3rd) based on Socio-Economic status\n# 2 Age\n# 3 Sex\n# 4 Survived -  1 if the passenger survived;  0 if the passenger did not survive\n# 5 PersonID\n\n# Label is a target variable. PersonInfo is a list of independent variables besides unique identifier\n\nLabeledDocument = Row(\"PersonID\", \"PersonInfo\", \"label\")\n\n# Define a function that parses the raw CSV file and returns an object of type LabeledDocument\n\ndef parseDocument(line):\n    values = [str(x) for x in line.split(',')] \n    if (values[4]>'0'):\n      Survived = 1.0\n    else:\n      Survived = 0.0\n        \n    textValue = str(values[1]) + \" \" + str(values[2])+\" \" + str(values[3])\n    return LabeledDocument(values[5], textValue, Survived)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"access_data\"></a>\n## Access Data\nBefore you can access data in the data file in the Object Storage, you must setup the Spark configuration with your Object Storage credentials. \n\nTo do this, click on the cell below and select the **Insert to code > Insert Spark Session DataFrame** function from the Files tab below the data file you want to work with."}, {"metadata": {}, "cell_type": "markdown", "source": "<div class=\"alert alert-block alert-info\">The following code contains the credentials for a file in your IBM Cloud Object Storage.</div>"}, {"metadata": {}, "cell_type": "code", "source": "# Object Storage Credentials", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"Explore\"></a>\n## Explore Data"}, {"metadata": {}, "cell_type": "code", "source": "print('Number of Passengers', df_data_1.count())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Number of passengers who survived and number passengers who died\ndf_data_1.groupby('Survived').count().show()\n#Number of passengers who survived and number passengers who died by gender\ndf_data_1.groupby('Survived', 'Sex').count().show()\n#Number of passengers who survived and number passengers who died by gender and ticket class\ndf_data_1.groupby('Survived', 'Sex', 'PClass').count().show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Number of passengers who survived and number passengers who died by gender\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf_data_1.crosstab('Sex', 'Survived').show()\ndf=df_data_1.crosstab('Sex', 'Survived').toPandas()\ndf.plot.bar(x=\"Sex_Survived\", legend=True , title=\"Survival by Gender\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Number of males and number females by ticket class\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf_data_1.crosstab('PClass', 'Sex').show()\ndf=df_data_1.crosstab('Pclass', 'Sex').toPandas()\ndf.plot.barh(x=\"Pclass_Sex\", legend=True , title=\"Gender by PClass\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"Parse\"></a>\n## Parse Data\nNow let's load the data into a `Spark RDD` and output the number of rows and first 5 rows.\nEach project you create has a bucket in your object storage. You may get the bucket name from the project Settings page. Replace the string `BUCKET` to the bucket name"}, {"metadata": {}, "cell_type": "code", "source": "data = sc.textFile(cos.url('Titanic.csv', 'BUCKET'))\nprint (\"Total records in the data set:\", data.count())\nprint (\"The first 5 rows\")\ndata.take(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create DataFrame from RDD"}, {"metadata": {}, "cell_type": "code", "source": "#Load the data into a dataframe, parse it using the function above\ndocuments = data.filter(lambda s: \"Name\" not in s).map(parseDocument)\nTitanicData = documents.toDF() # ToDataFrame\nprint (\"Number of records: \" + str(TitanicData.count()))\nprint (\"First 5 records: \")\nTitanicData.take(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"training_test\"></a>\n## Split Data into Training and Test set\n\nWe divide the data into training and test set.  The training set is used to build the model to be used on future data, and the test set is used to evaluate the model."}, {"metadata": {}, "cell_type": "code", "source": "# Divide the data into training and test set\n(train, test) = TitanicData.randomSplit([0.8, 0.2])\nprint (\"Number of records in the training set: \" + str(train.count()))\nprint (\"Number of records in the test set: \" + str(test.count()))\n# Output first 20 records in the training set\nprint (\"First 20 records in the training set: \")\ntrain.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"build_model\"></a>\n## Build Naive Bayes Model\n\nWe use the Pipeline of SparkML to build the Naive Bayes Model"}, {"metadata": {}, "cell_type": "code", "source": "# set up Naive Bayes using Pipeline of SparkML\ntokenizer = Tokenizer(inputCol=\"PersonInfo\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nnb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", predictionCol=\"prediction\", smoothing=1.0, modelType=\"multinomial\")\npipeline = Pipeline(stages=[tokenizer, hashingTF, nb])", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# set up Naive Bayes Model\n# the stages are executed in order\nmodel = pipeline.fit(train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"test_data\"></a>\n## Predict for Test data"}, {"metadata": {}, "cell_type": "code", "source": "# Make predictions for test data and print columns of interest\nprediction = model.transform(test)\nselected = prediction.select(\"PersonInfo\", \"prediction\", \"probability\")\nfor row in selected.collect():\n    print (row)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Tabulate the predicted outcome\nprediction.select(\"prediction\").groupBy(\"prediction\").count().show(truncate=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Tabulate the actual outcome\nprediction.select(\"label\").groupBy(\"label\").count().show(truncate=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# This table shows:\n# 1. The number of passengers who survived predicted as died\n# 2. The number of passengers who survived predicted as survived\n# 3. The number of passengers who died predicted as died\n# 4. The number of passengers who died predicted as survived\n\nprediction.crosstab('label', 'prediction').show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"evaluate_model\"></a>\n## Evaluate the Model\n\nWe evaluate the model on a training set and on a test set.  The purpose is to measure the model's predictive accuracy, including the accuracy for new data."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Evaluate the Naive Bayes model on a training set\n# Select (prediction, true label) and compute test error\npred_nb=model.transform(train).select(\"prediction\", \"label\")\neval_nb=MulticlassClassificationEvaluator (\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy_nb=eval_nb.evaluate(pred_nb)\n# create RDD\npredictionAndLabels_nb=pred_nb.rdd\nmetrics_nb=MulticlassMetrics(predictionAndLabels_nb)\nprecision_nb=metrics_nb.precision(1.0)\nrecall_nb=metrics_nb.recall(1.0)\nf1Measure_nb = metrics_nb.fMeasure(1.0, 1.0)\nprint (\"Model evaluation for the training data\")\nprint (\"Accuracy = %s\" %accuracy_nb)\nprint (\"Error = %s\" % (1-accuracy_nb))\nprint (\"Precision = %s\" %precision_nb)\nprint (\"Recall = %s\" %recall_nb)\nprint(\"F1 Measure = %s\" % f1Measure_nb)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Evaluate the Naive Bayes model on a test set\n# Select (prediction, true label) and compute test error\npred_nb=model.transform(test).select(\"prediction\", \"label\")\neval_nb=MulticlassClassificationEvaluator (\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy_nb=eval_nb.evaluate(pred_nb)\n# create RDD\npredictionAndLabels_nb=pred_nb.rdd\nmetrics_nb=MulticlassMetrics(predictionAndLabels_nb)\nprecision_nb=metrics_nb.precision(1.0)\nrecall_nb=metrics_nb.recall(1.0)\nf1Measure_nb = metrics_nb.fMeasure(1.0, 1.0)\nprint (\"Model evaluation for the test data\")\nprint (\"Test Accuracy = %s\" %accuracy_nb)\nprint (\"Test Error = %s\" % (1-accuracy_nb))\nprint (\"Precision = %s\" %precision_nb)\nprint (\"Recall = %s\" %recall_nb)\nprint(\"F1 Measure = %s\" % f1Measure_nb)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "bin_nb=BinaryClassificationMetrics(predictionAndLabels_nb)\n\n# Area under precision-recall curve\nprint(\"Area under PR = %s\" % bin_nb.areaUnderPR)\n# Area under precision-recall curve\nprint(\"Area under ROC = %s\" % bin_nb.areaUnderROC)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python37", "display_name": "Python 3.7 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.7.9", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}, "name": "machineLearningHAVCBluemix.ipynb"}, "nbformat": 4, "nbformat_minor": 1}